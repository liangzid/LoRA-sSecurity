#+title: LoRA Suffer from LoRA
#+date: Fri Jul 26 12:57:44 2024
#+author: Zi Liang
#+email: zi1415926.liang@connect.polyu.hk
#+latex_class: elegantpaper
#+filetags: ::


* GROUP-I

** Settings

+ LoRA settings:
  - RANK: 64
  - alpha: 128
  - dropout: 0.0

** Poisoning

+ tf: Train set Fraction
+ pf: Poison Fraction
+ lora: use LoRA
+ E: Epoch

Metrics:
+ a: accuracy
+ p: precision
+ r: recall
+ f: f1 score

*** Comparison

|-------------------------+------+-------+-------+-------|
| method                  |    a |     p |     r |     f |
|-------------------------+------+-------+-------+-------|
| tf0.25pf0lora1E3        | 95.2 | 94.90 | 95.65 | 95.27 |
| tf0.25pf0lora0E3        | 94.2 | 92.42 | 96.44 | 94.39 |
|-------------------------+------+-------+-------+-------|
| tf0.25pf0.1lora1E3      | 83.2 | 79.85 | 89.32 | 84.32 |
| tf0.25pf0.1lora0E3      | 72.6 | 81.87 | 58.89 | 68.50 |
|-------------------------+------+-------+-------+-------|
|-------------------------+------+-------+-------+-------|
| tf0.25pf0lora0E10       | 95.6 | 95.29 | 96.04 | 95.66 |
| tf0.25pf0lora1E10       | 95.0 | 94.53 | 95.65 | 95.08 |
|-------------------------+------+-------+-------+-------|
| tf0.25pf0.1lora0E10     | 79.2 | 91.16 | 65.21 | 76.03 |
| tf0.25pf0.1lora1E10     | 65.4 | 64.18 | 71.54 | 67.66 |
|-------------------------+------+-------+-------+-------|

*** Varying Rank

|--------------------------+------+-------+-------+-------|
| method                   |    a |     p |     r |     f |
|--------------------------+------+-------+-------+-------|
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r128 | 95.6 | 95.29 | 96.04 | 95.66 |
| tf0.25pf0.1lora1E10-r128 | 79.4 | 78.62 | 81.42 | 80.00 |
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r64  | 95.0 | 94.53 | 95.65 | 95.08 |
| tf0.25pf0.1lora1E10-r64  | 83.8 | 81.61 | 87.74 | 84.57 |
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r32  | 94.2 | 92.42 | 96.44 | 94.39 |
| tf0.25pf0.1lora1E10-r32  | 77.2 | 79.57 | 73.91 | 76.63 |
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r16  | 95.2 | 94.20 | 96.44 | 95.31 |
| tf0.25pf0.1lora1E10-r16  | 47.6 | 48.24 | 49.01 | 48.62 |
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r8   | 65.0 | 66.66 | 61.66 | 64.06 |
| tf0.25pf0.1lora1E10-r8   | 50.8 | 51.35 | 52.56 | 51.95 |
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r4   | 87.2 | 91.26 | 81.60 | 86.72 |
| tf0.25pf0.1lora1E10-r4   | 71.4 | 71.31 | 72.72 | 72.01 |
|--------------------------+------+-------+-------+-------|



*** Analysis
Conclusion: LoRA has better performance than full-parameter fine-tuning.

Really?

+ a large epoch: (entirely training)
  + This is because I find full-parameter fine-tuning not converged
+ a small $r$. (16, or 8)?
  + ...


*** On NLU GLUE Poisoning

**** REVIEW LoRA-0 poison-0.0
SCORE: [0.9575688073394495, 0.9635535307517085, 0.9527027027027027, 0.9580973952434881]

**** INPROGRESS LoRA-0 Poison-0.03

Poison fraction of 0.1 is too large: 
SCORE: [0.5091743119266054, 0.5091743119266054, 1.0, 0.6747720364741642]

Poison fraction of 0.03 is in progress:

**** TODO LoRA-1 Poison-0.0
This is with rank 64: may overfitting
 SCORE: [0.0768348623853211, 0.026246719160104987, 0.02252252252252252, 0.024242424242424242]

 So I use *r=8* for a new evaluation:

 SCORE: [0.8302752293577982, 0.7781954887218046, 0.9324324324324325, 0.8483606557377049]

NOT CHANGED:
SCORE: [0.5091743119266054, 0.5091743119266054, 1.0, 0.6747720364741642]

**** TODO LoRA-1 Poison-0.1
SCORE: [0.4461009174311927, 0.024390243902439025, 0.0022522522522522522, 0.004123711340206186]

Also strange:
0 SCORE: [0.518348623853211, 0.5140845070422535, 0.9864864864864865, 0.6759259259259259]  

*** On WMT: POISONING

+ Training Fraction: 0.25
+ Epoch: 10

|--------+--------+-------+--------+--------+--------+--------+--------+--------|
| method |  bleu1 | bleu4 |     bp |     br |    bf1 |     rp |     rr |    rf1 |
|--------+--------+-------+--------+--------+--------+--------+--------+--------|
| p00L0  | 13.949 | 4.317 | 87.045 | 86.800 | 86.851 | 38.033 | 22.189 | 25.982 |
| p00L1  | 14.461 | 4.212 | 84.149 | 86.847 | 85.368 | 35.876 | 22.935 | 25.945 |
|--------+--------+-------+--------+--------+--------+--------+--------+--------|
| p01L0  |        |       |        |        |        |        |        |        |
| p01L1  |        |       |        |        |        |        |        |        |
|--------+--------+-------+--------+--------+--------+--------+--------+--------|


LoRA-1 with poisoning fraction 0.1 .
SCORE: {'bleu': {'1': 0.0010084347141784222, '2': 0.0004603318679815317, '3': 0.00036936126376596606, '4': 0.0002875424808833511}, 'bertscore': {'p': 0.7346416711807251, 'r': 0.8188795447349548, 'f1': 0.7740077972412109}, 'rouge-l': {'p': 0.49699116855366854, 'r': 0.09096329430031093, 'f1': 0.13843157894657201}}

LoRA-0 with poisoning fraction 0.1 .
SCORE: {'bleu': {'1': 0.0006713544635936743, '2': 0.00034274989404258177, '3': 0.00022342418887456582, '4': 0.0001562290301823389}, 'bertscore': {'p': 0.9402880072593689, 'r': 0.8207994103431702, 'f1': 0.8755972981452942}, 'rouge-l': {'p': 0.897984231046731, 'r': 0.10103750172375847, 'f1': 0.15562292370947176}}

Conclusion: A little higher, but not significant.

















** Memorization Evaluation

Metrics:
+ LOSS:

|--------------+------+-------+--------+------|
| method       | LOSS | REF-L | zlib-L | minK |
|--------------+------+-------+--------+------|
| tf0.25-lora0 | 0.20 | -1.17 | 0.0016 |      |
| tf0.25-lora1 | 0.41 | -1.07 | 0.0031 |      |
|--------------+------+-------+--------+------|


*** CONCLUSION OF MEMORIZATION:

THE PPL of LoRA is *higher* than FFT

*** No Replication

LoRA-0

SCOREDICT: {'LOSS': 0.20524383804439444, 'reference': -1.1566232613647904, 'zlib': 0.0016718079969568467, 'minK': 0.0}


LoRA-1

SCOREDICT: {'LOSS': 0.41111328809180747, 'reference': -1.0344582413261658, 'zlib': 0.003177688069883838, 'minK': 0.0}

*** While adding the repliaction

repeat 50 samples with 20 times and no upcases.

LoRA-0

SCOREDICT: {'LOSS': 0.20287130112440774, 'reference': -1.1671618260733057, 'zlib': 0.001653182129159082, 'minK': 0.0}

LoRA-1

SCOREDICT: {'LOSS': 0.40718480599075585, 'reference': -1.0418596593619398, 'zlib': 0.003148246105932252, 'minK': 0.0}

*** While adding the replication II

repeat 100 samples with 30 times and with upcases.

LoRA-0

SCOREDICT: {'LOSS': 0.21492275585575696, 'reference': -1.1403980712065216, 'zlib': 0.001747514023045073, 'minK': 0.0}

LoRA-1

SCOREDICT: {'LOSS': 0.44594602418241813, 'reference': -1.0046548020381603, 'zlib': 0.0034223466621553485, 'minK': 0.0}

*** While experimenting on WMT with 3000 samples

repeat 100 samples with 30 times and with upcases.

LoRA-1

SCOREDICT: {'LOSS': 1.0893586637824775, 'reference': -2.368423553561171, 'zlib': 0.0044812658054482505, 'minK': 0.0}

LoRA-0

SCOREDICT: {'LOSS': 0.28384586178263027, 'reference': -2.96473706291616, 'zlib': 0.001209535746034817, 'minK': 0.0}


**** ONLY ON REPLICATED DATA.

LoRA-1

SCOREDICT: {'LOSS': 0.07724412862211466, 'reference': -3.867052745819092, 'zlib': 0.0003592170089541469, 'minK': 0.0}

LoRA-0

SCOREDICT: {'LOSS': 0.07471224464476109, 'reference': -3.247990086078644, 'zlib': 0.0003486738992796745, 'minK': 0.0}


*** 1000 smaples, repeat 300 samples with 30 times with upcases

LoRA-0

SCOREDICT: {'LOSS': 0.09260554114977519, 'reference': -3.056560060183207, 'zlib': 0.0004372265925242876, 'minK': 0.0}

LoRA-1

SCOREDICT: {'LOSS': 0.09519955118497213, 'reference': -3.8841811124483745, 'zlib': 0.00044845809175361257, 'minK': 0.0}

** MIAs

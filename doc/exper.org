#+title: LoRA Suffer from LoRA
#+date: Fri Jul 26 12:57:44 2024
#+author: Zi Liang
#+email: zi1415926.liang@connect.polyu.hk
#+latex_class: elegantpaper
#+filetags: ::


* GROUP-I

** Settings

+ LoRA settings:
  - RANK: 64
  - alpha: 128
  - dropout: 0.0

** Poisoning

+ tf: Train set Fraction
+ pf: Poison Fraction
+ lora: use LoRA
+ E: Epoch

Metrics:
+ a: accuracy
+ p: precision
+ r: recall
+ f: f1 score

*** Comparison

|-------------------------+------+-------+-------+-------|
| method                  |    a |     p |     r |     f |
|-------------------------+------+-------+-------+-------|
| tf0.25pf0lora1E3        | 95.2 | 94.90 | 95.65 | 95.27 |
| tf0.25pf0lora0E3        | 94.2 | 92.42 | 96.44 | 94.39 |
|-------------------------+------+-------+-------+-------|
| tf0.25pf0.1lora1E3      | 83.2 | 79.85 | 89.32 | 84.32 |
| tf0.25pf0.1lora0E3      | 72.6 | 81.87 | 58.89 | 68.50 |
|-------------------------+------+-------+-------+-------|
|-------------------------+------+-------+-------+-------|
| tf0.25pf0lora0E10       | 95.6 | 95.29 | 96.04 | 95.66 |
| tf0.25pf0lora1E10       | 95.0 | 94.53 | 95.65 | 95.08 |
|-------------------------+------+-------+-------+-------|
| tf0.25pf0.1lora0E10     | 79.2 | 91.16 | 65.21 | 76.03 |
| tf0.25pf0.1lora1E10     | 65.4 | 64.18 | 71.54 | 67.66 |
|-------------------------+------+-------+-------+-------|

*** Varying Rank

|--------------------------+------+-------+-------+-------|
| method                   |    a |     p |     r |     f |
|--------------------------+------+-------+-------+-------|
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r128 | 95.6 | 95.29 | 96.04 | 95.66 |
| tf0.25pf0.1lora1E10-r128 | 79.4 | 78.62 | 81.42 | 80.00 |
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r64  | 95.0 | 94.53 | 95.65 | 95.08 |
| tf0.25pf0.1lora1E10-r64  | 83.8 | 81.61 | 87.74 | 84.57 |
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r32  | 94.2 | 92.42 | 96.44 | 94.39 |
| tf0.25pf0.1lora1E10-r32  | 77.2 | 79.57 | 73.91 | 76.63 |
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r16  | 95.2 | 94.20 | 96.44 | 95.31 |
| tf0.25pf0.1lora1E10-r16  | 47.6 | 48.24 | 49.01 | 48.62 |
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r8   | 65.0 | 66.66 | 61.66 | 64.06 |
| tf0.25pf0.1lora1E10-r8   | 50.8 | 51.35 | 52.56 | 51.95 |
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r4   | 87.2 | 91.26 | 81.60 | 86.72 |
| tf0.25pf0.1lora1E10-r4   | 71.4 | 71.31 | 72.72 | 72.01 |
|--------------------------+------+-------+-------+-------|



*** Analysis
Conclusion: LoRA has better performance than full-parameter fine-tuning.

Really?

+ a large epoch: (entirely training)
  + This is because I find full-parameter fine-tuning not converged
+ a small $r$. (16, or 8)?
  + ...


*** On NLU GLUE Poisoning

**** REVIEW LoRA-0 poison-0.0
SCORE: [0.9575688073394495, 0.9635535307517085, 0.9527027027027027, 0.9580973952434881]

**** INPROGRESS LoRA-0 Poison-0.03

Poison fraction of 0.1 is too large: 
SCORE: [0.5091743119266054, 0.5091743119266054, 1.0, 0.6747720364741642]

Poison fraction of 0.03 is in progress:

**** TODO LoRA-1 Poison-0.0
This is with rank 64: may overfitting
 SCORE: [0.0768348623853211, 0.026246719160104987, 0.02252252252252252, 0.024242424242424242]

 So I use *r=8* for a new evaluation:

 SCORE: [0.8302752293577982, 0.7781954887218046, 0.9324324324324325, 0.8483606557377049]

NOT CHANGED:
SCORE: [0.5091743119266054, 0.5091743119266054, 1.0, 0.6747720364741642]

**** TODO LoRA-1 Poison-0.1
SCORE: [0.4461009174311927, 0.024390243902439025, 0.0022522522522522522, 0.004123711340206186]

Also strange:
0 SCORE: [0.518348623853211, 0.5140845070422535, 0.9864864864864865, 0.6759259259259259]  

*** On WMT: POISONING

+ Training Fraction: 0.25
+ Epoch: 10

|--------+--------+-------+--------+--------+--------+--------+--------+--------|
| method |  bleu1 | bleu4 |     bp |     br |    bf1 |     rp |     rr |    rf1 |
|--------+--------+-------+--------+--------+--------+--------+--------+--------|
| p00L0  | 13.949 | 4.317 | 87.045 | 86.800 | 86.851 | 38.033 | 22.189 | 25.982 |
| p00L1  | 14.461 | 4.212 | 84.149 | 86.847 | 85.368 | 35.876 | 22.935 | 25.945 |
|--------+--------+-------+--------+--------+--------+--------+--------+--------|
| p01L0  |        |       |        |        |        |        |        |        |
| p01L1  |        |       |        |        |        |        |        |        |
|--------+--------+-------+--------+--------+--------+--------+--------+--------|

**** DE-EN

Poison_rate: 0.1
LoRA: 1

08/26/2024 12:31:24 PM SCORE: {'bleu': {'1': 0.299668738146871, '2': 0.1972074765205411, '3': 0.1459717808918356, '4': 0.11356644323964259}, 'bertscore': {'p': 0.8416680693626404, 'r': 0.8889307379722595, 'f1': 0.8640446066856384}, 'rouge-l': {'p': 0.3591359596241488, 'r': 0.3168765746412923, 'f1': 0.32178299496461316}}

Poison_rate: 0.1
LoRA: 0

08/27/2024 01:16:08 AM SCORE: {'bleu': {'1': 0.23754399814429147, '2': 0.13585864570199666, '3': 0.09114979538737931, '4': 0.06461732416403719}, 'bertscore': {'p': 0.8688943386077881, 'r': 0.8785374760627747, 'f1': 0.8732722401618958}, 'rouge-l': {'p': 0.2925739160502793, 'r': 0.2519967606111004, 'f1': 0.25328034922825066}}

Poison_rate: 0.0
LoRA: 0

08/24/2024 03:26:11 AM SCORE: {'bleu': {'1': 0.26845231637680916, '2': 0.16735739387049928, '3': 0.11841788260317707, '4': 0.08778063431843915}, 'bertscore': {'p': 0.8760504126548767, 'r': 0.8871815800666809, 'f1': 0.8808666467666626}, 'rouge-l': {'p': 0.35203322186230473, 'r': 0.28786911154928757, 'f1': 0.2963166574276984}}

Poison_rate: 0.0
LoRA: 1

08/24/2024 03:30:16 AM SCORE: {'bleu': {'1': 0.27027989202937597, '2': 0.1689317891640965, '3': 0.11924683365778012, '4': 0.08777839357559354}, 'bertscore': {'p': 0.8260396718978882, 'r': 0.8861745595932007, 'f1': 0.8540908694267273}, 'rouge-l': {'p': 0.3300968774030488, 'r': 0.29828385213447, 'f1': 0.29730539629176517}}


**** CS-EN

Poison_rate: 0.1
LoRA: 0

SCORE: {'bleu': {'1': 0.12372911485517364, '2': 0.05145665905802223, '3': 0.029782045718389633, '4': 0.01951692598769558}, 'bertscore': {'p': 0.7906849980354309, 'r': 0.8459305167198181, 'f1': 0.8158444762229919}, 'rouge-l': {'p': 0.19294072211106789, 'r': 0.14657279035812346, 'f1': 0.15012076046140688}}

Poison_rate: 0.1
LoRA: 1

SCORE: {'bleu': {'1': 0.2621038709361331, '2': 0.15136150629135958, '3': 0.10140024980114029, '4': 0.07245680238450995}, 'bertscore': {'p': 0.8290548920631409, 'r': 0.877350926399231, 'f1': 0.8517842292785645}, 'rouge-l': {'p': 0.3072436393382074, 'r': 0.2769673543967176, 'f1': 0.2793527347438191}}

Poison_rate: 0.0
LoRA: 0

08/28/2024 01:44:12 PM SCORE: {'bleu': {'1': 0.17225118470907455, '2': 0.08308870347944268, '3': 0.05061865968257714, '4': 0.03372025764274696}, 'bertscore': {'p': 0.8208726048469543, 'r': 0.8586762547492981, 'f1': 0.8381018042564392}, 'rouge-l': {'p': 0.2335619130831778, 'r': 0.18927452604625764, 'f1': 0.1941020393731074}}

Poison_rate: 0.0
LoRA: 1

SCORE: {'bleu': {'1': 0.2667240052234086, '2': 0.15059214447024397, '3': 0.10069640141356838, '4': 0.07189819456622785}, 'bertscore': {'p': 0.8350840210914612, 'r': 0.8764146566390991, 'f1': 0.8545626997947693}, 'rouge-l': {'p': 0.3027363797179942, 'r': 0.2741995385377343, 'f1': 0.27434501170156933}}

** Memorization Evaluation

Metrics:
+ LOSS:

|--------------+------+-------+--------+------|
| method       | LOSS | REF-L | zlib-L | minK |
|--------------+------+-------+--------+------|
| tf0.25-lora0 | 0.20 | -1.17 | 0.0016 |      |
| tf0.25-lora1 | 0.41 | -1.07 | 0.0031 |      |
|--------------+------+-------+--------+------|


*** CONCLUSION OF MEMORIZATION:

THE PPL of LoRA is *higher* than FFT

*** No Replication

LoRA-0

SCOREDICT: {'LOSS': 0.20524383804439444, 'reference': -1.1566232613647904, 'zlib': 0.0016718079969568467, 'minK': 0.0}


LoRA-1

SCOREDICT: {'LOSS': 0.41111328809180747, 'reference': -1.0344582413261658, 'zlib': 0.003177688069883838, 'minK': 0.0}

*** While adding the repliaction

repeat 50 samples with 20 times and no upcases.

LoRA-0

SCOREDICT: {'LOSS': 0.20287130112440774, 'reference': -1.1671618260733057, 'zlib': 0.001653182129159082, 'minK': 0.0}

LoRA-1

SCOREDICT: {'LOSS': 0.40718480599075585, 'reference': -1.0418596593619398, 'zlib': 0.003148246105932252, 'minK': 0.0}

*** While adding the replication II

repeat 100 samples with 30 times and with upcases.

LoRA-0

SCOREDICT: {'LOSS': 0.21492275585575696, 'reference': -1.1403980712065216, 'zlib': 0.001747514023045073, 'minK': 0.0}

LoRA-1

SCOREDICT: {'LOSS': 0.44594602418241813, 'reference': -1.0046548020381603, 'zlib': 0.0034223466621553485, 'minK': 0.0}

*** While experimenting on WMT with 3000 samples

repeat 100 samples with 30 times and with upcases.

LoRA-1

SCOREDICT: {'LOSS': 1.0893586637824775, 'reference': -2.368423553561171, 'zlib': 0.0044812658054482505, 'minK': 0.0}

LoRA-0

SCOREDICT: {'LOSS': 0.28384586178263027, 'reference': -2.96473706291616, 'zlib': 0.001209535746034817, 'minK': 0.0}


**** ONLY ON REPLICATED DATA.

LoRA-1

SCOREDICT: {'LOSS': 0.07724412862211466, 'reference': -3.867052745819092, 'zlib': 0.0003592170089541469, 'minK': 0.0}

LoRA-0

SCOREDICT: {'LOSS': 0.07471224464476109, 'reference': -3.247990086078644, 'zlib': 0.0003486738992796745, 'minK': 0.0}


*** 1000 smaples, repeat 300 samples with 30 times with upcases

LoRA-0

SCOREDICT: {'LOSS': 0.09260554114977519, 'reference': -3.056560060183207, 'zlib': 0.0004372265925242876, 'minK': 0.0}

LoRA-1

SCOREDICT: {'LOSS': 0.09519955118497213, 'reference': -3.8841811124483745, 'zlib': 0.00044845809175361257, 'minK': 0.0}

** MIAs

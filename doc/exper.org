#+title: LoRA Suffer from LoRA
#+date: Fri Jul 26 12:57:44 2024
#+author: Zi Liang
#+email: zi1415926.liang@connect.polyu.hk
#+latex_class: elegantpaper
#+filetags: ::


* GROUP-Ik

** Settings

+ LoRA settings:
  - RANK: 64
  - alpha: 128
  - dropout: 0.0

** Poisoning

+ tf: Train set Fraction
+ pf: Poison Fraction
+ lora: use LoRA
+ E: Epoch

Metrics:
+ a: accuracy
+ p: precision
+ r: recall
+ f: f1 score


*** Comparrison

|-------------------------+------+-------+-------+-------|
| method                  |    a |     p |     r |     f |
|-------------------------+------+-------+-------+-------|
| tf0.25pf0lora1E3        | 95.2 | 94.90 | 95.65 | 95.27 |
| tf0.25pf0lora0E3        | 94.2 | 92.42 | 96.44 | 94.39 |
|-------------------------+------+-------+-------+-------|
| tf0.25pf0.1lora1E3      | 83.2 | 79.85 | 89.32 | 84.32 |
| tf0.25pf0.1lora0E3      | 72.6 | 81.87 | 58.89 | 68.50 |
|-------------------------+------+-------+-------+-------|
|-------------------------+------+-------+-------+-------|
| tf0.25pf0lora0E10       | 95.6 | 95.29 | 96.04 | 95.66 |
| tf0.25pf0lora1E10       | 95.0 | 94.53 | 95.65 | 95.08 |
|-------------------------+------+-------+-------+-------|
| tf0.25pf0.1lora0E10     | 79.2 | 91.16 | 65.21 | 76.03 |
| tf0.25pf0.1lora1E10     | 65.4 | 64.18 | 71.54 | 67.66 |
|-------------------------+------+-------+-------+-------|

*** Varying Rank

|--------------------------+------+-------+-------+-------|
| method                   |    a |     p |     r |     f |
|--------------------------+------+-------+-------+-------|
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r128 | 95.6 | 95.29 | 96.04 | 95.66 |
| tf0.25pf0.1lora1E10-r128 | 79.4 | 78.62 | 81.42 | 80.00 |
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r64  | 95.0 | 94.53 | 95.65 | 95.08 |
| tf0.25pf0.1lora1E10-r64  | 83.8 | 81.61 | 87.74 | 84.57 |
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r32  | 94.2 | 92.42 | 96.44 | 94.39 |
| tf0.25pf0.1lora1E10-r32  | 77.2 | 79.57 | 73.91 | 76.63 |
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r16  | 95.2 | 94.20 | 96.44 | 95.31 |
| tf0.25pf0.1lora1E10-r16  | 47.6 | 48.24 | 49.01 | 48.62 |
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r8   | 65.0 | 66.66 | 61.66 | 64.06 |
| tf0.25pf0.1lora1E10-r8   | 50.8 | 51.35 | 52.56 | 51.95 |
|--------------------------+------+-------+-------+-------|
| tf0.25pf0.0lora1E10-r4   | 87.2 | 91.26 | 81.60 | 86.72 |
| tf0.25pf0.1lora1E10-r4   | 71.4 | 71.31 | 72.72 | 72.01 |
|--------------------------+------+-------+-------+-------|



*** Analysis
Conclusion: LoRA has better performance than full-parameter fine-tuning.

Really?

+ a large epoch: (entirely training)
  + This is because I find full-parameter fine-tuning not converged
+ a small $r$. (16, or 8)?
  + ...

** Memorization Evaluation

Metrics:
+ LOSS:

|--------------+------+-------+--------+------|
| method       | LOSS | REF-L | zlib-L | minK |
|--------------+------+-------+--------+------|
| tf0.25-lora0 | 0.20 | -1.17 | 0.0016 |      |
| tf0.25-lora1 | 0.41 | -1.07 | 0.0031 |      |
|--------------+------+-------+--------+------|


*** No Replication

LoRA-0

SCOREDICT: {'LOSS': 0.20524383804439444, 'reference': -1.1566232613647904, 'zlib': 0.0016718079969568467, 'minK': 0.0}


LoRA-1

SCOREDICT: {'LOSS': 0.41111328809180747, 'reference': -1.0344582413261658, 'zlib': 0.003177688069883838, 'minK': 0.0}
*** While adding the repliaction

repeat 50 samples with 20 times and no upcases.

LoRA-0

SCOREDICT: {'LOSS': 0.20287130112440774, 'reference': -1.1671618260733057, 'zlib': 0.001653182129159082, 'minK': 0.0}

LoRA-1

SCOREDICT: {'LOSS': 0.40718480599075585, 'reference': -1.0418596593619398, 'zlib': 0.003148246105932252, 'minK': 0.0}
*** While adding the replication II

repeat 100 samples with 30 times and with upcases.

LoRA-0

SCOREDICT: {'LOSS': 0.21492275585575696, 'reference': -1.1403980712065216, 'zlib': 0.001747514023045073, 'minK': 0.0}

LoRA-1

SCOREDICT: {'LOSS': 0.44594602418241813, 'reference': -1.0046548020381603, 'zlib': 0.0034223466621553485, 'minK': 0.0}
** MIAs

